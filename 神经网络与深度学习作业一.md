
## <div align="center">多层前馈网络与误差反传算法</div>
#### 一、多层感知机
##### 1.1 XOR问题
线性不可分问题：无法进行线性分类。

<p align="center">
  <img src="E:\obsidian\lifeos\3. 资源\研究生课程\研一下\神经网络与深度学习\作业1\线性不可分1.png" width="35%">
  <img src="E:\obsidian\lifeos\3. 资源\研究生课程\研一下\神经网络与深度学习\作业1\线性不可分2.png" width="33%">
</p>

##### 1.2多层感知机
针对XOR问题提出的解决方法是使用<b>多层感知机 </b>
•在输入和输出层间加一或多层隐单元，构成多层感知器（多层前馈神经网络）。
•加一层隐节点（单元）为三层网络，可解决异或（XOR）问题。由输入得到两个隐节点、一个输出层节点的输出。
#### 二、多层前馈网络以及BP算法概述
##### 2.1 多层前馈网络
多层感知机是一种<b>多层前馈网络</b>，由多层神经网络构成，每层网络将输出传递给下一层网络。神经元间的权值连接仅出现在相邻层之间，不出现在其他位置。如果每一个神经元都连接到上一层的所有神经元（除输入层外），则成为全连接网络。
多层前馈网络的反向传播 （BP）学习算法，简称BP算法，是有导师的学习，它是梯度下降法在多层前馈网中的应用。
<b>网络结构</b>:𝐮（或𝐱 ）、 𝐲是网络的输入、输出向量，神经元用节点表示，网络由输入层、隐层和输出层节点组成，隐层可一层，也可多层（图中是单隐层），前层至后层节点通过权联接。由于用BP学习算法，所以常称BP神经网络。

##### 2.2 BP算法概述
➢已知网络的输入/输出样本，即导师信号。
➢ BP学习算法由正向传播和反向传播组成：
① 正向传播是输入信号从输入层经隐层，传向输出层，若输出层得到了期望的输出，则学习算法结束；否则，转至反向传播。
② 反向传播是将误差(样本输出与网络输出之差）按原联接通路反向计算，由梯度下降法调整各层节点的权值和阈值，使误差减小。
#### 三、BP算法详解
##### 3.1 BP算法基本思想
BP算法是训练神经网络的核心方法，通过计算损失函数对参数的梯度并更新权重，以最小化预测误差。其过程分为以下步骤：

1.前向传播：
输入数据逐层传递，每层进行线性变换（权重矩阵相乘加偏置）并通过激活函数（如Sigmoid、ReLU）生成输出，直至输出层得到预测值。
2.计算损失：
使用损失函数（如均方误差、交叉熵）衡量预测值与真实值的差距，得到标量损失值。
3.反向传播梯度：
输出层梯度：计算损失对输出层输入的梯度（损失函数导数）。
逐层反向传递：从输出层开始，利用链式法则计算各层参数的梯度。
梯度 = 上一层梯度 × 激活函数导数 × 前一层输出（对权重）或直接传递（对偏置）。
链式法则：将梯度从后向前传递，依次计算各层权重和偏置的梯度。
4.参数更新
使用优化算法（如梯度下降）按学习率调整参数：
$$W=W-\eta\cdot\frac{\partial\mathrm{Loss}}{\partial W},\quad b=b-\eta\cdot\frac{\partial\mathrm{Loss}}{\partial b}$$
其中，\(\eta\) 为学习率，控制更新步长。


<b>关键点</b>
激活函数需可导（如ReLU在非零点可导），否则无法计算梯度。
链式法则是核心，确保高效计算高维参数梯度。
挑战：梯度消失/爆炸（深层网络中梯度可能过小或过大）。
BP算法通过迭代“前向预测→反向调参”优化网络，广泛应用于多层感知机、CNN、RNN等模型训练。

##### 3.2 程序示例
```python
import numpy as np

# 激活函数及其导数（使用 sigmoid）
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# 输入数据（XOR）
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

# 目标输出
y = np.array([[0],
              [1],
              [1],
              [0]])

# 初始化权重
np.random.seed(42)
input_dim = 2
hidden_dim = 4
output_dim = 1

W1 = np.random.randn(input_dim, hidden_dim)
b1 = np.zeros((1, hidden_dim))
W2 = np.random.randn(hidden_dim, output_dim)
b2 = np.zeros((1, output_dim))

# 学习率
eta = 0.1

# 训练迭代
for epoch in range(10000):
    # 前向传播
    Z1 = X @ W1 + b1
    A1 = sigmoid(Z1)
    Z2 = A1 @ W2 + b2
    A2 = sigmoid(Z2)

    # 计算损失（可选）
    loss = np.mean((y - A2) ** 2)

    # 反向传播
    dA2 = (A2 - y) * sigmoid_derivative(A2)
    dW2 = A1.T @ dA2
    db2 = np.sum(dA2, axis=0, keepdims=True)

    dA1 = dA2 @ W2.T * sigmoid_derivative(A1)
    dW1 = X.T @ dA1
    db1 = np.sum(dA1, axis=0, keepdims=True)

    # 更新权重
    W2 -= eta * dW2
    b2 -= eta * db2
    W1 -= eta * dW1
    b1 -= eta * db1

    # 每1000次打印一次损失
    if epoch % 1000 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# 最终预测输出
print("Final output:")
print(A2)
```
##### 3.3 程序使用数据集
<b>Fashion-MNIST数据集</b>
➢ FashionMNIST 是一个替代 MNIST 手写数字集 的图像数据集。它是由 Zalando旗下的研究部门提供， 涵盖了来自 10 种类别的共 7 万个不同商品的正面图片。
➢ FashionMNIST 的大小、 格式和训练集/测试集划分与原始的MNIST 完全一致。 60000/10000 的训练测试数据划分， 28x28的灰度图片。 你可以直接用它来测试你的机器学习和深度学习算法性能， 且不需要改动任何的代码。
<b>MNIST数据集加载</b>

```python
def load_data_fashion_mnist(batch_size, resize=None): #@save
"""下载Fashion-MNIST数据集，然后将其加载到内存中。 """
  trans = [transforms.ToTensor()]
  if resize:
    trans.insert(0, transforms.Resize(resize))
  trans = transforms.Compose(trans)
  mnist_train = torchvision.datasets.FashionMNIST(
    root="../data", train=True, transform=trans, download=True)
  mnist_test = torchvision.datasets.FashionMNIST(
    root="../data", train=False, transform=trans, download=True)
  return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                          num_workers=get_dataloader_workers()),
          data.DataLoader(mnist_test, batch_size, shuffle=False,
                          num_workers=get_dataloader_workers()))
```
## <div align="center">性能优化</div>
#### 一、常用技巧
1. 模型初始化
• 简单初始化：将权重在[-1,1]区间内按均值或高斯分布初始化。
• Xavier初始化：使每一层输出的方差尽量相等，采用均匀分布。
2. 数据划分
• 数据分为训练数据、验证数据和测试数据，比例通常为70%、15%、15%或60%、20%、20%。
• K折交叉验证：将原始训练数据分成K个子集，每次在K-1个子集上训练，在剩余子集上验证，最后取平均值。
3. 欠拟合与过拟合
• 欠拟合：误差一直较大。
• 过拟合：在训练数据集上误差小，而在测试数据集上误差大。
4. 权重衰减（L2正则化）
• 在损失函数中加入权重的L2范数作为正则化项，防止过拟合。
5. Dropout
• 在训练过程中随机将某些节点置零，减少过拟合。
#### 二、动量法
• 问题：SGD在病态曲率区域（如山谷状区域）收敛速度慢，容易在山脊处反弹。
• 动量法原理：通过引入动量参数，使优化过程更加稳定，类似于将纸团换成铁球，减少震荡。
• 更新公式：
$$\begin{aligned}u_{t}&=\alpha u_{t-1}-\eta g_t\\\Delta\theta&=u_t\end{aligned}$$

其中，\(\alpha\)是动量参数，\(\eta\)是学习率，\(g\)是梯度。
#### 三、自适应梯度算法
1. AdaGrad
• 参数自适应变化，学习率与历史梯度平方值总和的平方根成反比。
• 问题：学习率单调递减，后期可能过小。
2. RMSProp
• 解决AdaGrad学习率过快衰减的问题，引入指数衰减平均。
• 更新公式：
$$\begin{aligned}r_{t}&=\rho r_{t-1}+(1-\rho)g_t^2\\\Delta\theta&=-\eta\frac{g_t}{\sqrt{r_t+\epsilon}}\end{aligned}$$

其中，\(\rho\)是衰减速率，\(\epsilon\)是小常数，用于数值稳定。
3. Adam
• 结合了动量法和RMSProp的优点，同时保留了历史梯度的指数衰减平均和历史梯度平方的指数衰减平均。
• 更新公式：


## <div align="center">卷积神经网络基础</div>
#### 一、为什么要“深度学习”
##### 1.1全连接网络问题
<b>全连接网络</b>：链接权过多，算的慢，难收敛，同时可能进入局部极小值，也容易产生过拟合问题。
<b>解决算的慢问题</b>：减少权值连接，每一个节点只连到上一层的少
数神经元，即局部连接网络。
<b>解决难收敛、算的慢问题</b>：权值过多极易产生过拟合。如何消除？
<b>回想人类解决思路</b>：信息分层处理，每一层在上层提取特征的基础上获取进行再处理，得到更高级别的特征。

##### 1.2 深度学习平台简介
| 库名 | 发布者 | 支持语言 | 支持系统 |
|:------:|:-------:|:-------:|:-------:|
|TensorFlow| Google |Python/C++/Java/Go| Linux/Mac OS/Android/iOS|
|Caffe| UC Berkeley |Python/C++/Matlab| Linux/Mac OS/Windows
|JAX |Google |Python| Linux/Windows|
| MXNet | Amazon/DMLC(分布式机器学习社区)|Python/C++/Matlab/Julia/Go/R/Scala|Linux/Mac OS/Windows/Android/iOS|
| Torch/PyTorch| Facebook| C/Python/..| Linux/Mac OS/Windows/Android/iOS|
| PaddlePaddle |百度 |Python |Linux/Windows|
| MMdetection |商汤/港中文 |Python| Linux/Windows|

##### 1.3 PyTorch简介与基本使用

➢ 张量（Tensor）
是一个物理量，对高维 (维数 ≥ 2) 的物理量进行“量纲分析” 的一种工具。 简单的可以理解为：一维数组称为矢量， 二维数组为二阶张量， 三维数组为三阶张量 …
➢ 计算图
用“ 结点” （nodes）和“ 线” (edges)的有向图来描述数学计算的图像。 “节点” 一般用来表示施加的数学操作， 但也可以表示数据输入的起点/输出的终点， 或者是读取/写入持久变量的终点。 “线” 表示“节点” 之间的输入/输出关系。 这些数据“线”可以输运“ size可动态调整” 的多维数据数组， 即“张量” （tensor）

➢ 使用 tensor 表示数据
➢ 使用 Dataset、 DataLoader 读取样本数据和标签
➢ 使用变量 (Variable) 存储神经网络权值等参数
➢ 使用计算图 (computational graph) 来表示计算任务
➢ 在代码运行过程中同时执行计算图
##### 1.4PyTorch基本使用-简单示例
构建简单的计算图，每个节点将零个或多个tensor作为输入，产生一个tensor作为输出。 PyTorch中，所见即为所得， tensor的使用和numpy中的多维数组类似：
```python
import torch
x_const = torch.tensor([1.0, 2.0, 3.0])
y = torch.tensor([3.0, 4.0, 5.0])
output = x_const + y
print(x_const, '\n', y, '\n',output)

tensor([1.0, 2.0, 3.0])
tensor([3.0, 4.0, 5.0])
tensor([4.0, 6.0, 8.0])
```
#### 二、卷积神经网络基础
##### 2.1 基本概念

卷积神经网络（Convolutional Neural Network, CNN）是一种专门用于处理具有类似网格结构的数据（如图像）的深度学习模型。CNN 能够自动提取特征，并通过层级结构逐步学习数据的抽象表示。

🧩 一、特征提取（卷积层）
CNN 的核心在于卷积操作，它通过**卷积核（filter）**在输入数据上滑动，对局部区域进行加权求和，从而提取空间局部特征。
- 卷积核一般尺寸为 $3\times3$、$5\times5$ 等；
- 每个卷积核学习提取特定类型的特征（如边缘、纹理）；
- 多个卷积核可以学习不同的特征图（feature maps）；
- 卷积操作具有**参数共享**和**局部连接**优势。

⚙️ 二、激活函数（Activation Function）
卷积之后通常会使用非线性激活函数，引入非线性能力，使网络能拟合更复杂的函数。
常见激活函数：
- ReLU（Rectified Linear Unit）：$f(x) = \max(0, x)$，简单高效，最常用；
- Sigmoid、Tanh：早期使用较多，现已较少用于卷积层中；
- Leaky ReLU、ELU：改进 ReLU 避免“神经元死亡”问题。

🧱 三、网络结构组成

一个典型的 CNN 网络结构由以下部分组成：
1. **输入层（Input Layer）**  
   接受原始图像数据（如 $32\times32\times3$ 的彩色图像）
2. **卷积层（Conv Layer）**  
   提取图像的局部特征，输出多个特征图
3. **激活层（ReLU Layer）**  
   添加非线性
4. **池化层（Pooling Layer）**  
   降维并增强特征不变性，常用 max pooling 或 average pooling
5. **全连接层（Fully Connected Layer）**  
   将抽象特征映射到具体任务（如分类）
6. **输出层（Output Layer）**  
   输出最终结果，分类时通常使用 Softmax

🏗️ 四、CNN 的层级特征提取能力
- **底层卷积层**：提取边缘、角点等低级特征
- **中间卷积层**：提取纹理、形状等结构信息
- **高层卷积层**：识别具体对象的语义特征

##### 2.2 误差反向传播
误差反向传播算法是训练神经网络的核心算法之一，主要用于计算各层权重的梯度，从而通过优化方法（如梯度下降）进行权重更新，使得模型的预测值逐渐逼近真实值。


🔁 一、基本思想
BP 算法的基本思想是：
1. 前向传播（Forward Propagation）：  
   输入数据从输入层传到输出层，计算预测值。
2. 计算误差（Loss）：  
   通过损失函数衡量预测值与真实标签的误差。
3. 反向传播（Backward Propagation）：  
   从输出层开始，逐层计算误差对每层权重的梯度，使用链式法则（链式求导）。
4. 参数更新（Update）：  
   利用学习率 $\eta$ 和梯度，按如下公式更新参数：
   $$
   w := w - \eta \cdot \frac{\partial L}{\partial w}
   $$

🧮 二、数学原理（以全连接网络为例）

- 假设损失函数为 $L$，网络输出为 $\hat{y}$，真实标签为 $y$
- 网络中的每一层输出为：
  $$
  z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)} \\
  a^{(l)} = f(z^{(l)})
  $$
- 反向传播从最后一层开始，逐层向前计算误差项 $\delta^{(l)}$：
  $$
  \delta^{(L)} = \nabla_a L \odot f'(z^{(L)}) \\
  \delta^{(l)} = (W^{(l+1)})^T \delta^{(l+1)} \odot f'(z^{(l)})
  $$

- 梯度公式为：
  $$
  \frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T \\
  \frac{\partial L}{\partial b^{(l)}} = \delta^{(l)}
  $$

⚙️ 三、关键要素

- **链式法则**：BP 的核心，逐层传播误差信号
- **激活函数导数**：必须能求导，如 ReLU、Sigmoid、Tanh 等
- **权重共享**：卷积神经网络中可简化 BP 操作
- **梯度消失/爆炸**：深层网络中 BP 可能面临的问题


🧠 四、简化流程

1. 初始化参数（权重、偏置）
2. 对每个样本：
   - 前向传播 → 预测值
   - 计算损失
   - 反向传播 → 求梯度
   - 参数更新（如 SGD）
3. 重复多个 epoch，直到收敛
#### 三、LeNet-5 网络结构与原理总结

LeNet-5 是 Yann LeCun 于 1998 年提出的卷积神经网络结构，是最早成功应用于手写数字识别的 CNN 模型之一。该模型主要用于对 MNIST 数据集中的 $32\times32$ 灰度手写数字图像进行分类。


##### 3.1 网络结构总览
LeNet-5 网络由 **7 层**构成（不包括输入层），包含：
- 卷积层（Convolutional Layer）
- 池化层（Subsampling/Pooling Layer）
- 全连接层（Fully Connected Layer）

> 输入图像大小：$32\times32$  
> 输出：10 类（数字 0–9）

##### 3.2 结构详解（每层功能）

| 层级 | 名称     | 类型        | 输出尺寸     | 参数说明                          |
|------|----------|-------------|--------------|-----------------------------------|
| 1    | Input    | -           | 32×32×1      | 输入灰度图像                     |
| 2    | C1       | 卷积层      | 28×28×6      | 6 个 5×5 卷积核，步长为 1        |
| 3    | S2       | 池化层      | 14×14×6      | 每个特征图 2×2 平均池化，步长为 2 |
| 4    | C3       | 卷积层      | 10×10×16     | 部分连接，6→16 个特征图          |
| 5    | S4       | 池化层      | 5×5×16       | 2×2 平均池化                     |
| 6    | C5       | 卷积层      | 1×1×120      | 卷积核尺寸为 5×5×16              |
| 7    | F6       | 全连接层    | 84           | 与 C5 全连接                     |
| 8    | Output   | 全连接层    | 10           | 使用 Softmax 分类为 10 类        |

##### 3.3 层间连接说明
- **C1 卷积层**提取边缘、纹理等局部特征；
- **S2 池化层**对特征图进行降采样，增强平移不变性；
- **C3 卷积层**增加特征提取的复杂性，非完全连接减少参数数量；
- **S4 池化层**继续降维；
- **C5 实质是全连接卷积层**，将空间信息整合为高层抽象；
- **F6 和 Output 层**负责最终的分类任务。
  
#### 四、基本卷积神经网络
##### 4.1 AlexNet
网络说明
网络一共有8层可学习层——5层卷积层和3层全连接层
**➢ 改进**
-池化层均采用最大池化
-选用ReLU作为非线性环节激活函数
-网络规模扩大，参数数量接近6000万
-出现“多个卷积层+一个池化层”的结构
**➢ 普遍规律**
-随网络深入，宽、高衰减，通道数增加
改进：输入样本
➢ 最简单、通用的图像数据变形的方式
• 从原始图像（256,256）中，随机的crop出一些图像（224,224）。 【平移变换， crop】
• 水平翻转图像。 【反射变换， flip】
• 给图像增加一些随机的光照。 【光照、彩色变换， color jittering】
**改进：激活函数**
➢ 采用ReLU替代 Tan Sigmoid
➢ 用于卷积层与全连接层之后
**改进： Dropout**
➢ 在每个全连接层后面使用一个 Dropout 层,以概率 p 随机关闭激活函数
**改进：双GPU策略**
➢ AlexNet使用两块GTX580显卡进行训练，两块显卡只需要在特定的层进行通信
##### 4.2 VGG-16
VGG-16 是由牛津大学视觉几何组（Visual Geometry Group）在 2014 年提出的一种深度卷积神经网络。其在 ImageNet 图像分类任务中表现出色，具有统一且简洁的网络结构，是深度卷积网络发展的里程碑之一。
**网络结构概述**

VGG-16 共包含 **16 层有权重的网络层**，其中包括：

- 13 个卷积层（Conv Layers）
- 3 个全连接层（Fully Connected Layers）

所有卷积层均使用 **$3\times3$ 的小卷积核**，池化层使用 **$2\times2$ 的最大池化（Max Pooling）**。

> 输入图像尺寸：$224\times224\times3$

**VGG-16 网络结构表**

| 阶段 | 层类型         | 输出尺寸             | 卷积核数与大小   |
|------|----------------|----------------------|------------------|
| 输入 | -              | 224×224×3            | -                |
| 1    | Conv3-64       | 224×224×64           | 2 层，$3\times3$ |
|      | MaxPool        | 112×112×64           | $2\times2$, s=2  |
| 2    | Conv3-128      | 112×112×128          | 2 层，$3\times3$ |
|      | MaxPool        | 56×56×128            | $2\times2$, s=2  |
| 3    | Conv3-256      | 56×56×256            | 3 层，$3\times3$ |
|      | MaxPool        | 28×28×256            | $2\times2$, s=2  |
| 4    | Conv3-512      | 28×28×512            | 3 层，$3\times3$ |
|      | MaxPool        | 14×14×512            | $2\times2$, s=2  |
| 5    | Conv3-512      | 14×14×512            | 3 层，$3\times3$ |
|      | MaxPool        | 7×7×512              | $2\times2$, s=2  |
| FC   | 全连接层       | 4096 → 4096 → 1000   | 3 层 FC + Softmax|

**网络设计特点**
1. **统一的卷积核尺寸**  
   所有卷积层均使用 $3\times3$ 卷积核，便于网络结构的模块化和堆叠。
2. **加深网络深度**  
   通过堆叠多个小卷积层来增加感受野和非线性表达能力，避免用大卷积核带来的计算开销。
3. **固定池化策略**  
   所有池化层均为 $2\times2$ 最大池化，步长为 2，有效降低特征图尺寸。
4. **全连接层处理高层抽象信息**  
   最后三层全连接层用于综合高层特征并进行分类。

##### 4.3 残差网络 ResNet（残差网络）结构与原理总结

ResNet（Residual Network）是微软研究院于 2015 年提出的一种深度卷积神经网络，在 ImageNet 竞赛中取得了冠军，并首次成功训练超过 100 层的网络。其核心思想是引入 **“残差连接（Residual Connection）”** 以解决深层网络训练难的问题。

**提出动机：解决深层网络退化问题**
- 随着网络加深，模型的表现并不总是变好，甚至会**退化**（训练误差反而变大）；
- 这不是过拟合，而是由于**梯度消失/爆炸**、**信息阻塞**等问题导致的训练困难。
> ResNet 通过**跳跃连接（skip connection）**将前一层的输入直接加到后面的输出上，使网络更容易学习到恒等映射，解决退化问题。


**残差块（Residual Block）结构**

每个残差模块包含两个或三个卷积层，其核心公式为：

$$
\text{输出} = \mathcal{F}(x) + x
$$

- 其中 $\mathcal{F}(x)$ 表示通过卷积层学习的残差映射；
- $x$ 是输入特征，直接加到 $\mathcal{F}(x)$ 上；
- 如果维度不一致，通常通过 $1\times1$ 卷积调整维度。

残差块结构示意图：

```
输入 x
 ↓
Conv → BN → ReLU → Conv → BN
 ↓
加上跳跃连接 x
 ↓
ReLU
```
**典型结构（以 ResNet-18 为例）**

| 模块       | 层结构                                  | 输出尺寸     |
|------------|------------------------------------------|--------------|
| Conv1      | 7×7 卷积, 64通道, 步长2 + MaxPool        | 112×112×64   |
| Conv2_x    | 2个残差块，每块包含 2 个 3×3 卷积        | 56×56×64     |
| Conv3_x    | 2个残差块，输出通道加倍，空间尺寸减半    | 28×28×128    |
| Conv4_x    | 2个残差块                                | 14×14×256    |
| Conv5_x    | 2个残差块                                | 7×7×512      |
| 全局平均池化 | GAP → FC（1000分类）                     | 1×1×512 → 1000|

**深层变种**

- ResNet-18 / ResNet-34：使用 **BasicBlock**
- ResNet-50 / ResNet-101 / ResNet-152：使用 **BottleneckBlock**（带 1×1 卷积压缩与扩展）

**ResNet 的优势**

- ✅ 支持训练极深的网络（>100 层）
- ✅ 显著提升图像识别准确率（Top-5 error < 5%）
- ✅ 加速收敛，提升训练稳定性
- ✅ 易于扩展与迁移（广泛用于目标检测、语义分割、视频分析等任务）



